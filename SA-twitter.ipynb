{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "AI2: HW1 Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWvGPpg9f5Se",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Artificial Intelligence II: HW1 Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hviYDW1Pfyip",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ],
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7gngNBcJXQC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "KOrpXNGTg5fJ",
    "outputId": "e2c5088d-617d-4364-8388-742bc44633be",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_df = pd.read_csv(\"VaccineSentimentDataset/vaccine_train_set.csv\", index_col=0)\n",
    "train_df"
   ],
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   tweet  label\n0      Sip N Shop Come thru right now #Marjais #Popul...      0\n1      I don't know about you but My family and I wil...      1\n2      @MSignorile Immunizations should be mandatory....      2\n3      President Obama spoke in favor of vaccination ...      0\n4      \"@myfoxla: Arizona monitoring hundreds for mea...      0\n...                                                  ...    ...\n15971  @Salon if u believe the anti-vax nutcases caus...      1\n15972  How do you feel about parents who don't #vacci...      0\n15973  70 Preschoolers Tested for Measles in Simi Val...      0\n15974  Finance Minister: Budget offers room to procur...      0\n15975  Are you up to date on vaccines? Take CDC’s vac...      2\n\n[15976 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sip N Shop Come thru right now #Marjais #Popul...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I don't know about you but My family and I wil...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@MSignorile Immunizations should be mandatory....</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>President Obama spoke in favor of vaccination ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"@myfoxla: Arizona monitoring hundreds for mea...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15971</th>\n      <td>@Salon if u believe the anti-vax nutcases caus...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15972</th>\n      <td>How do you feel about parents who don't #vacci...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15973</th>\n      <td>70 Preschoolers Tested for Measles in Simi Val...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15974</th>\n      <td>Finance Minister: Budget offers room to procur...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15975</th>\n      <td>Are you up to date on vaccines? Take CDC’s vac...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>15976 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "              label\ncount  15976.000000\nmean       0.936592\nstd        0.930740\nmin        0.000000\n25%        0.000000\n50%        1.000000\n75%        2.000000\nmax        2.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>15976.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.936592</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.930740</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  tweet  label\n0     @user They had a massive surge in with covid d...      1\n1     Required vaccines for school: Parents and guar...      0\n2     “@KCStar: Two more Johnson County children hav...      0\n3     NV can do better. Which states are the best (a...      2\n4     Nothing like killing ourselves w/ our own fear...      2\n...                                                 ...    ...\n2277  RT @abc7: Number of measles cases reported in ...      0\n2278  Evidence points to the idea that \"measles affe...      0\n2279  Where's @SavedYouAClick \"@voxdotcom: Why you s...      2\n2280  Some of my favorite people have autism. If tha...      2\n2281  Coronavirus: The married couple behind the suc...      0\n\n[2282 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@user They had a massive surge in with covid d...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Required vaccines for school: Parents and guar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>“@KCStar: Two more Johnson County children hav...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NV can do better. Which states are the best (a...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Nothing like killing ourselves w/ our own fear...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2277</th>\n      <td>RT @abc7: Number of measles cases reported in ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2278</th>\n      <td>Evidence points to the idea that \"measles affe...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2279</th>\n      <td>Where's @SavedYouAClick \"@voxdotcom: Why you s...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2280</th>\n      <td>Some of my favorite people have autism. If tha...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2281</th>\n      <td>Coronavirus: The married couple behind the suc...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2282 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"VaccineSentimentDataset/vaccine_validation_set.csv\", index_col=0)\n",
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "             label\ncount  2282.000000\nmean      0.936897\nstd       0.930960\nmin       0.000000\n25%       0.000000\n50%       1.000000\n75%       2.000000\nmax       2.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2282.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.936897</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.930960</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "sn.countplot(x='label',data=train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sn.countplot(x='label',data=test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0njGkifQ-zrX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uc2REn8u9w7b",
    "outputId": "a8021260-d31f-42e9-ec3b-e1321e626944",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#Check for null values\n",
    "train_df.isnull().sum()"
   ],
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "tweet    0\nlabel    0\ndtype: int64"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "tweet    0\nlabel    0\ndtype: int64"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Separating features from targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gyUAcnM__MRK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "X_train = train_df.drop('label', axis=1)\n",
    "Y_train = train_df['label']\n",
    "X_train"
   ],
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   tweet\n0      Sip N Shop Come thru right now #Marjais #Popul...\n1      I don't know about you but My family and I wil...\n2      @MSignorile Immunizations should be mandatory....\n3      President Obama spoke in favor of vaccination ...\n4      \"@myfoxla: Arizona monitoring hundreds for mea...\n...                                                  ...\n15971  @Salon if u believe the anti-vax nutcases caus...\n15972  How do you feel about parents who don't #vacci...\n15973  70 Preschoolers Tested for Measles in Simi Val...\n15974  Finance Minister: Budget offers room to procur...\n15975  Are you up to date on vaccines? Take CDC’s vac...\n\n[15976 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sip N Shop Come thru right now #Marjais #Popul...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I don't know about you but My family and I wil...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@MSignorile Immunizations should be mandatory....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>President Obama spoke in favor of vaccination ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"@myfoxla: Arizona monitoring hundreds for mea...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15971</th>\n      <td>@Salon if u believe the anti-vax nutcases caus...</td>\n    </tr>\n    <tr>\n      <th>15972</th>\n      <td>How do you feel about parents who don't #vacci...</td>\n    </tr>\n    <tr>\n      <th>15973</th>\n      <td>70 Preschoolers Tested for Measles in Simi Val...</td>\n    </tr>\n    <tr>\n      <th>15974</th>\n      <td>Finance Minister: Budget offers room to procur...</td>\n    </tr>\n    <tr>\n      <th>15975</th>\n      <td>Are you up to date on vaccines? Take CDC’s vac...</td>\n    </tr>\n  </tbody>\n</table>\n<p>15976 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "0        0\n1        1\n2        2\n3        0\n4        0\n        ..\n15971    1\n15972    0\n15973    0\n15974    0\n15975    2\nName: label, Length: 15976, dtype: int64"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  tweet\n0     @user They had a massive surge in with covid d...\n1     Required vaccines for school: Parents and guar...\n2     “@KCStar: Two more Johnson County children hav...\n3     NV can do better. Which states are the best (a...\n4     Nothing like killing ourselves w/ our own fear...\n...                                                 ...\n2277  RT @abc7: Number of measles cases reported in ...\n2278  Evidence points to the idea that \"measles affe...\n2279  Where's @SavedYouAClick \"@voxdotcom: Why you s...\n2280  Some of my favorite people have autism. If tha...\n2281  Coronavirus: The married couple behind the suc...\n\n[2282 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@user They had a massive surge in with covid d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Required vaccines for school: Parents and guar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>“@KCStar: Two more Johnson County children hav...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NV can do better. Which states are the best (a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Nothing like killing ourselves w/ our own fear...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2277</th>\n      <td>RT @abc7: Number of measles cases reported in ...</td>\n    </tr>\n    <tr>\n      <th>2278</th>\n      <td>Evidence points to the idea that \"measles affe...</td>\n    </tr>\n    <tr>\n      <th>2279</th>\n      <td>Where's @SavedYouAClick \"@voxdotcom: Why you s...</td>\n    </tr>\n    <tr>\n      <th>2280</th>\n      <td>Some of my favorite people have autism. If tha...</td>\n    </tr>\n    <tr>\n      <th>2281</th>\n      <td>Coronavirus: The married couple behind the suc...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2282 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_df.drop('label', axis=1)\n",
    "Y_test = test_df['label']\n",
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "0       1\n1       0\n2       0\n3       2\n4       2\n       ..\n2277    0\n2278    0\n2279    2\n2280    2\n2281    0\nName: label, Length: 2282, dtype: int64"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Text pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38215/455184214.py:29: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['tweet'] = dataset['tweet'].str.replace('[^\\w\\s]', '')  # replace(r'^https?:\\/\\/.*[\\r?\\n\\t@]*', '')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [98]\u001B[0m, in \u001B[0;36m<cell line: 55>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;66;03m# remove symbols\u001B[39;00m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# df['Comment'] = df['Comment'].replace(to_replace =r'([^\\s\\w]|_)+', value = '', regex = True)\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m X_train\n",
      "Input \u001B[0;32mIn [98]\u001B[0m, in \u001B[0;36mpreprocess\u001B[0;34m(dataset)\u001B[0m\n\u001B[1;32m     32\u001B[0m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(removeStopwords)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# removes most common words\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m#cnt = findMostCommonWords(dataset)\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m#dataset['tweet'] = dataset[\"tweet\"].apply(removeCommonWords)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m \n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# spelling correction\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtweet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTextBlob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# for entry in dataset:\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m#     # remove whitespaces\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m#     entry = entry.strip()\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m#     # remove stopwords and tokenize\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m#     tokens = word_tokenize(entry)\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m#     entry = [i for i in tokens if not i in stopwords]\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/pandas/core/series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4332\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/pandas/core/apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[1;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/pandas/core/apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[1;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[1;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[0;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Input \u001B[0;32mIn [98]\u001B[0m, in \u001B[0;36mpreprocess.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     32\u001B[0m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(removeStopwords)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# removes most common words\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m#cnt = findMostCommonWords(dataset)\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m#dataset['tweet'] = dataset[\"tweet\"].apply(removeCommonWords)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m \n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# spelling correction\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mstr\u001B[39m(\u001B[43mTextBlob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# for entry in dataset:\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m#     # remove whitespaces\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m#     entry = entry.strip()\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m#     # remove stopwords and tokenize\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m#     tokens = word_tokenize(entry)\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m#     entry = [i for i in tokens if not i in stopwords]\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/blob.py:609\u001B[0m, in \u001B[0;36mBaseBlob.correct\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    607\u001B[0m tokens \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mtokenize\u001B[38;5;241m.\u001B[39mregexp_tokenize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+|[^\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    608\u001B[0m corrected \u001B[38;5;241m=\u001B[39m (Word(w)\u001B[38;5;241m.\u001B[39mcorrect() \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens)\n\u001B[0;32m--> 609\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorrected\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(ret)\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/blob.py:608\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;66;03m# regex matches: word or punctuation or whitespace\u001B[39;00m\n\u001B[1;32m    607\u001B[0m tokens \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mtokenize\u001B[38;5;241m.\u001B[39mregexp_tokenize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+|[^\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 608\u001B[0m corrected \u001B[38;5;241m=\u001B[39m (\u001B[43mWord\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens)\n\u001B[1;32m    609\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(corrected)\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(ret)\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/blob.py:142\u001B[0m, in \u001B[0;36mWord.correct\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcorrect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;124;03m'''Correct the spelling of the word. Returns the word with the highest\u001B[39;00m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;124;03m    confidence using the spelling corrector.\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Word(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspellcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/blob.py:134\u001B[0m, in \u001B[0;36mWord.spellcheck\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mspellcheck\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;124;03m'''Return a list of (word, confidence) tuples of spelling corrections.\u001B[39;00m\n\u001B[1;32m    127\u001B[0m \n\u001B[1;32m    128\u001B[0m \u001B[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msuggest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstring\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/en/__init__.py:123\u001B[0m, in \u001B[0;36msuggest\u001B[0;34m(w)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msuggest\u001B[39m(w):\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;124;03m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mspelling\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuggest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/_text.py:1399\u001B[0m, in \u001B[0;36mSpelling.suggest\u001B[0;34m(self, w)\u001B[0m\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m w\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misdigit():\n\u001B[1;32m   1396\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(w, \u001B[38;5;241m1.0\u001B[39m)] \u001B[38;5;66;03m# 1.5\u001B[39;00m\n\u001B[1;32m   1397\u001B[0m candidates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_known([w]) \\\n\u001B[1;32m   1398\u001B[0m           \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_known(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_edit1(w)) \\\n\u001B[0;32m-> 1399\u001B[0m           \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_known(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_edit2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m) \\\n\u001B[1;32m   1400\u001B[0m           \u001B[38;5;129;01mor\u001B[39;00m [w]\n\u001B[1;32m   1401\u001B[0m candidates \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget(c, \u001B[38;5;241m0.0\u001B[39m), c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m candidates]\n\u001B[1;32m   1402\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28msum\u001B[39m(p \u001B[38;5;28;01mfor\u001B[39;00m p, word \u001B[38;5;129;01min\u001B[39;00m candidates) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/_text.py:1376\u001B[0m, in \u001B[0;36mSpelling._edit2\u001B[0;34m(self, w)\u001B[0m\n\u001B[1;32m   1372\u001B[0m \u001B[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001B[39;00m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001B[39;00m\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001B[39;00m\n\u001B[0;32m-> 1376\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_edit1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_edit1\u001B[49m\u001B[43m(\u001B[49m\u001B[43me1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/_text.py:1376\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1372\u001B[0m \u001B[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001B[39;00m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001B[39;00m\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001B[39;00m\n\u001B[0;32m-> 1376\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mset\u001B[39m(e2 \u001B[38;5;28;01mfor\u001B[39;00m e1 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_edit1(w) \u001B[38;5;28;01mfor\u001B[39;00m e2 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_edit1(e1) \u001B[38;5;28;01mif\u001B[39;00m \u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m)\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/_text.py:96\u001B[0m, in \u001B[0;36mlazydict.__contains__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__contains__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lazy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m__contains__\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/AI1/venv/lib/python3.8/site-packages/textblob/_text.py:87\u001B[0m, in \u001B[0;36mlazydict._lazy\u001B[0;34m(self, method, *args)\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, method, types\u001B[38;5;241m.\u001B[39mMethodType(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mdict\u001B[39m, method), \u001B[38;5;28mself\u001B[39m))\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mdict\u001B[39m, method)(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from Preprocess import *\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = preprocess(X_test)\n",
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vectorize words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#with unigrams without any optimization for now\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['tweet'])\n",
    "\n",
    "train_vec = vectorizer.transform(X_train['tweet'])\n",
    "print(vectorizer.get_feature_names())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#with unigrams without any optimization for now\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_test['tweet'])\n",
    "\n",
    "test_vec = vectorizer.transform(X_test['tweet'])\n",
    "print(vectorizer.get_feature_names())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMiIvmQKBSDm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-65fnO6RBWU3",
    "outputId": "de5f0fee-b44c-4f22-87f3-db129f13dd2b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#We will experiment with Ridge Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vec, Y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33fbT1geB4vG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TE5UCDlB7hz",
    "outputId": "e6f688da-319e-4590-e903-32108ea57978",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Now that we have trained the classifer, we can make predictions on the unseen data\n",
    "Y_test_pred = clf.predict(test_vec)\n",
    "print(Y_test_pred)\n",
    "# Let's also make predictions on the train set for reference\n",
    "Y_train_pred = clf.predict(train_vec)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mraBMAivCR8d",
    "outputId": "f5b37c30-7d26-4a3d-91b3-34a5ade706ea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "test_mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(f\"Our classifier achieves a MSE of {test_mse:.2f} on the test set\")\n",
    "train_mse = mean_squared_error(Y_train, Y_train_pred)\n",
    "print(f\"Our classifier achieves a MSE of {train_mse:.2f} on the train set\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "821WmssMIdbX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "* Scaling features\n",
    "* Experimenting with different models \n",
    "* Using different hyperparameters for each model\n",
    "* Testing which of the features are really helpful\n",
    "* Creating additional synthetic features\n",
    "* And many more...\n"
   ]
  }
 ]
}